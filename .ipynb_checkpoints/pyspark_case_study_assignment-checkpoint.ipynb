{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_PREFIX = \"https://merkle-de-interview-case-study.s3.eu-central-1.amazonaws.com/de/\"\n",
    "API_SUFFIX = \".csv\"\n",
    "NAME_ARRAY = [\"user\",\"order\",\"item\",\"event\"]\n",
    "PATH = \"C:\\\\Users\\\\GANESH\\\\Desktop\\\\pyspark\\\\API_FETCH_DATA\\\\\"\n",
    "API = \"http://web.cs.wpi.edu/~cs1004/a16/Resources/SacramentoRealEstateTransactions\"\n",
    "S3_PATH = \"s3a://spark/csv/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe105afd",
   "metadata": {},
   "source": [
    "### Dendencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyspark\n",
    "from pyspark.sql.functions import udf, col, explode\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72b705",
   "metadata": {},
   "source": [
    "### Download Data From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91af6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make API request, get response object back, create dataframe from above schema.\n",
    "res=None\n",
    "for name in NAME_ARRAY:\n",
    "    try:\n",
    "    #       res = requests.request('get', API_PREFIX + name + API_SUFFIX)\n",
    "        res = requests.request('get', API+API_SUFFIX)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"e\") \n",
    "\n",
    "    if res != None and res.status_code == 200:\n",
    "        with open(PATH+name+API_SUFFIX, \"wb\")as file:\n",
    "            for line in res.iter_lines(delimiter=b'\\\\n'):\n",
    "                file.write(line)\n",
    "                print(f\"SUCCESSFULLY DOWNLOAD: {name+API_SUFFIX}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95e1fe",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c8b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config('spark.app.name','s3app').\\\n",
    "config('spark.jars.packages','org.apache.hadoop:hadoop-aws:2.7.3,org.apache.hadoop:hadoop-common:2.7.3').\\\n",
    "getOrCreate()\n",
    "print(\"SparkSession Created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6939bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark configuration\n",
    "sc=spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e058e22f",
   "metadata": {},
   "source": [
    "### HADOOP Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.access.key', 'access_key')\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.secret.key', 'secret_key')\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.endpoint', 's3-us-east-2.amazonaws.com')\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b5172",
   "metadata": {},
   "source": [
    "### Read Data From S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda34c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_df=spark.read.csv(\"s3a://bucket_name/dummy.csv\",header=True,inferSchema=True)\n",
    "print(s3_df.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52768d92",
   "metadata": {},
   "source": [
    "### Load Data to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05021767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "for name in NAME_ARRAY:\n",
    "    spark.read.format('csv').options(header='true', inferSchema='true').load(S3_PATH+name+API_SUFFIX)\n",
    "\n",
    "    \n",
    "#2\n",
    "for name in NAME_ARRAY:\n",
    "df2.write.options(\"header\",\"true\").csv(S3_PATH+name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf205dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"SparkSession stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66de07",
   "metadata": {},
   "source": [
    "### LAYER 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707953d4",
   "metadata": {},
   "source": [
    "***Contains external tables for all prerequisite files.\\\n",
    "All attributes are of STRING type. No transformations are applied***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274320d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE event_raw(\n",
    "     event_id  STRING,\n",
    "     event_time STRING,\n",
    "     user_id STRING,\n",
    "     event.payload STRING\n",
    "                     );\n",
    "                 \n",
    "CREATE TABLE user_raw(\n",
    "    created_at STRING,\n",
    "    deleted_at STRING,\n",
    "    email_address STRING,\n",
    "    first_name STRING,\n",
    "    id STRING,\n",
    "    last_name STRING,\n",
    "    merged_at STRING,\n",
    "    parent_user_id STRING\n",
    "                     );\n",
    "\n",
    "CREATE TABLE order_raw(            \n",
    "    InvoiceId STRING,\n",
    "    LineItemId STRING,\n",
    "    UserId STRING,\n",
    "    ItemId STRING,\n",
    "    ItemName STRING,\n",
    "    ItemCategory STRING,\n",
    "    Price STRING,\n",
    "    CreatedAt STRING,\n",
    "    PaidAt STRING\n",
    "                 );\n",
    "                 \n",
    "CREATE TABLE item_raw(                \n",
    "    adjective STRING,\n",
    "    category STRING,\n",
    "    created_at STRING,\n",
    "    id STRING,\n",
    "    modifier STRING,\n",
    "    name STRING,\n",
    "    price STRING  \n",
    "                 );            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd4aee",
   "metadata": {},
   "source": [
    "### LAYER 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f9a9c",
   "metadata": {},
   "source": [
    "***▪ Contains all datasets from the first layer \\\n",
    "▪ All attributes have common naming convention \\\n",
    "▪ All attributes have proper datatypes based on the attribute name and\n",
    "common logic\\\n",
    "▪ All struct collection attributes are flattened and transformed to proper data\n",
    "types\\\n",
    "▪ Fact tables are properly partitioned based on meaningful attributes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to Check\n",
    "# expload at event.payload\n",
    "# {\"event_name\":\"view_item\",\"platform\":\"android\",\"parameter_name\":\"item_id\",\"parameter_value\":\"3526\"}\n",
    "# Fact table is needed to create  \n",
    "    \n",
    "\n",
    "CREATE TABLE event_raw_input AS(\n",
    "    \n",
    "     SELECT \n",
    "             event_id STRING,\n",
    "             CAST(event_time AS TIMESTAMP(‘MM/DD/YYYY HH:MI:ss) ) event_time,\n",
    "             CAST(user_id AS UNSIGNED ) user_id, \n",
    "             CAST expload(event.payload AS STRING) event,\n",
    "             CAST(event.event_name.value AS STRING) event_name,\n",
    "             CAST(event.platform.value AS STRING) platform,\n",
    "             CAST(event.parameter_name.value AS STRING) parameter_name,\n",
    "             CAST(event.parameter_value.value AS STRING) parameter_value,\n",
    "             \n",
    "    FROM event_raw\n",
    "                              );\n",
    "                 \n",
    "CREATE TABLE user_raw_input AS(\n",
    "    \n",
    "    SELECT\n",
    "        CAST(created_at AS  TIMESTAMP(‘MM/DD/YYYY HH:MI:ss)) created_at ,\n",
    "        CAST(deleted_at AS  TIMESTAMP(‘MM/DD/YYYY HH:MI:ss) ) deleted_at,\n",
    "        email_address STRING,\n",
    "        first_name STRING,\n",
    "        CAST(id AS UNSIGNED) user_id, \n",
    "        last_name STRING,\n",
    "        CAST(merged_at AS  TIMESTAMP(‘MM/DD/YYYY HH:MI:ss)) merged_at,\n",
    "        CAST(parent_user_id AS UNSIGNED) parent_user_id\n",
    "    FROM user_raw \n",
    "                     );\n",
    "\n",
    "\n",
    "CREATE TABLE order_raw_input( \n",
    "    \n",
    "    SELECT\n",
    "            CAST(InvoiceId AS UNSIGNED) invoice_id,\n",
    "            CAST(LineItemId AS UNSIGNED) line_item_id,\n",
    "            CAST(UserId AS UNSIGNED) user_id\n",
    "            CAST(ItemId AS UNSIGNED) item_id,\n",
    "            CAST(ItemName AS STRING) item_name,\n",
    "            CAST(ItemCategory AS STRING) item_category,\n",
    "            CAST(Price AS FLOAT) price,\n",
    "            CAST(CreatedAt AS TIMESTAMP(‘MM/DD/YYYY HH:MI:ss)) created_at ,\n",
    "            CAST(PaidAt AS TIMESTAMP(‘MM/DD/YYYY HH:MI:ss)) paid_at ,\n",
    "    FROM order_raw\n",
    "                            );\n",
    "                 \n",
    "CREATE TABLE item_raw_input(  \n",
    "    \n",
    "    SELECT\n",
    "            adjective STRING,\n",
    "            CAST(category AS STRING) item_category,\n",
    "            CAST(created_at AS TIMESTAMP(‘MM/DD/YYYY HH:MI:ss)) created_at ,\n",
    "            CAST(id AS UNSIGNED) item_id,\n",
    "            modifier STRING,\n",
    "            CAST(name AS STRING) item_name,\n",
    "           CAST(price AS FLOAT) price \n",
    "    FROM item_raw\n",
    "                            );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de69705",
   "metadata": {},
   "source": [
    "### LAYER 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec590f3d",
   "metadata": {},
   "source": [
    "***▪ Contains following data marts:***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a3014",
   "metadata": {},
   "source": [
    "***• “top_items” data mart with all sold items with additional attributes:***\\\n",
    "o For every year (based on the created_at attribute):\\\n",
    "     ▪ Total number of an items sold in a particular year\\\n",
    "     ▪ Rank of an item based on the total number of items sold in a particular year\\\n",
    "     ▪ Total sales from an item in a particular year\\\n",
    "     ▪ Rank of an item based on the total sales in a particular year\\\n",
    "o Total number of items sold in all years\\\n",
    "o Rank of an item based on the total number of solds\\\n",
    "o Total sales of an item in all years\\\n",
    "o Rank of an item based on the total sales\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c7eb3",
   "metadata": {},
   "source": [
    "### For every year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09aa650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df.select('item_id','item_name','line_item_id','created_at') where year == 2021\n",
    "\n",
    "# # ▪ Total number of an items sold in a particular year\n",
    "# # ▪ Rank of an item based on the total number of items sold in a particular year\n",
    "\n",
    "# total_sold=spark_df.groupBy([\"item_name\"]).agg(\n",
    "#     count(\"item_name\").alias(\"total_items_sold\"))\n",
    "# rank_of_item = total_sold.sort(desc('rank_of_item')) \n",
    "\n",
    "# # ▪ Total sales from an item in a particular year\n",
    "# # ▪ Rank of an item based on the total sales in a particular year\n",
    "\n",
    "# total_sales=spark_df.groupBy([\"item_name\"]).agg(\n",
    "#    sum(\"price\").alias(\"total_items_sales\"))\n",
    "# rank_of_item = total_sold.sort(desc('rank_of_item')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02b226",
   "metadata": {},
   "source": [
    "### For all year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df.select('item_id','item_name','line_item_id','created_at') where year == 2021\n",
    "\n",
    "# # o Total number of items sold in all years\n",
    "# # o Rank of an item based on the total number of solds\n",
    "\n",
    "\n",
    "# total_sold=spark_df.groupBy([\"item_name\"]).agg(\n",
    "#     count(\"item_name\").alias(\"total_items_sold\"))\n",
    "# rank_of_item = total_sold.sort(desc('rank_of_item')) \n",
    "\n",
    "# # o Total sales of an item in all years\n",
    "# # o Rank of an item based on the total sales\n",
    "\n",
    "# total_sales=spark_df.groupBy([\"item_name\"]).agg(\n",
    "#    sum(\"price\").alias(\"total_items_sales\"))\n",
    "# rank_of_item = total_sold.sort(desc('rank_of_item')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d7ff8",
   "metadata": {},
   "source": [
    "***• “top_buyers” data mart with top 20 customers who contributed on the total sales the most with additional attributes:***\\\n",
    "o Total sales contributed\\\n",
    "o Rank based on the total sales\\\n",
    "o Last order creation date\\\n",
    "o The overall most viewed item of a customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88566fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users=spark_df.groupBy([\"user_id\"]).agg(count(\"item_id\"),sum('price').alias(\"items_order_by_user\"))\n",
    "# rank_of_item = total_sold.sort(desc('items_order_by_user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707bd83",
   "metadata": {},
   "source": [
    "### Read data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(PATH+'user'+API_SUFFIX, inferSchema = True, header = True)\n",
    "df.show(6,False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df\n",
    "xgysvsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5702e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
