{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f16ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_PREFIX = \"https://merkle-de-interview-case-study.s3.eu-central-1.amazonaws.com/de/\"\n",
    "API_SUFFIX = \".csv\"\n",
    "NAME_ARRAY = [\"user\",\"order\",\"item\",\"event\"]\n",
    "PATH = \"C:\\\\Users\\\\GANESH\\\\Desktop\\\\pyspark\\\\API_FETCH_DATA\\\\\"\n",
    "API = \"http://web.cs.wpi.edu/~cs1004/a16/Resources/SacramentoRealEstateTransactions\"\n",
    "S3_PATH = \"s3a://spark/csv/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe105afd",
   "metadata": {},
   "source": [
    "### Dendencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ccc7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import requests\n",
    "import pyspark\n",
    "from pyspark.sql.functions import udf, col, explode\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72b705",
   "metadata": {},
   "source": [
    "### Download Data From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91af6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make API request, get response object back, create dataframe from above schema.\n",
    "res=None\n",
    "for name in NAME_ARRAY:\n",
    "    try:\n",
    "    #       res = requests.request('get', API_PREFIX + name + API_SUFFIX)\n",
    "        res = requests.request('get', API+API_SUFFIX)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"e\") \n",
    "\n",
    "    if res != None and res.status_code == 200:\n",
    "        with open(PATH+name+API_SUFFIX, \"wb\")as file:\n",
    "            for line in res.iter_lines(delimiter=b'\\\\n'):\n",
    "                file.write(line)\n",
    "                print(f\"SUCCESSFULLY DOWNLOAD: {name+API_SUFFIX}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95e1fe",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9c8b6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1120)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1106)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:547)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1574)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:498)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:498)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:498)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:325)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 21 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11692/4226825213.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.app.name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m's3app'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark.jars.packages'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'org.apache.hadoop:hadoop-aws:2.7.3,org.apache.hadoop:hadoop-common:2.7.3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SparkSession Created successfully\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0m\u001b[0;32m    136\u001b[0m                           conf, jsc, profiler_cls)\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[1;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m--> 317\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1567\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1568\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1569\u001b[0m             answer, self._gateway_client, None, self._fqn)\n\u001b[0;32m   1570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1120)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1106)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:547)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1574)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:498)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:498)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:498)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:325)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 21 more\r\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.app.name','s3app').\\\n",
    "config('spark.jars.packages','org.apache.hadoop:hadoop-aws:2.7.3,org.apache.hadoop:hadoop-common:2.7.3').\\\n",
    "getOrCreate()\n",
    "print(\"SparkSession Created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6939bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark configuration\n",
    "sc=spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e058e22f",
   "metadata": {},
   "source": [
    "### HADOOP Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.access.key', 'access_key')\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.secret.key', 'secret_key')\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.endpoint', 's3-us-east-2.amazonaws.com')\n",
    "hadoopConf.set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b5172",
   "metadata": {},
   "source": [
    "### Read Data From S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda34c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_df=spark.read.csv(\"s3a://bucket_name/dummy.csv\",header=True,inferSchema=True)\n",
    "print(s3_df.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52768d92",
   "metadata": {},
   "source": [
    "### Load Data to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05021767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "for name in NAME_ARRAY:\n",
    "    spark.read.format('csv').options(header='true', inferSchema='true').load(S3_PATH+name+API_SUFFIX)\n",
    "\n",
    "    \n",
    "#2\n",
    "for name in NAME_ARRAY:\n",
    "df2.write.options(\"header\",\"true\").csv(S3_PATH+name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf205dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"SparkSession stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66de07",
   "metadata": {},
   "source": [
    "### LAYER 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707953d4",
   "metadata": {},
   "source": [
    "***Contains external tables for all prerequisite files.\\\n",
    "All attributes are of STRING type. No transformations are applied***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274320d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE event_raw(\n",
    "     event_id  STRING,\n",
    "     event_time STRING,\n",
    "     user_id STRING,\n",
    "     event.payload STRING\n",
    "                     );\n",
    "                 \n",
    "CREATE TABLE user_raw(\n",
    "    created_at STRING,\n",
    "    deleted_at STRING,\n",
    "    email_address STRING,\n",
    "    first_name STRING,\n",
    "    id STRING,\n",
    "    last_name STRING,\n",
    "    merged_at STRING,\n",
    "    parent_user_id STRING\n",
    "                     );\n",
    "\n",
    "CREATE TABLE order_raw(            \n",
    "    InvoiceId STRING,\n",
    "    LineItemId STRING,\n",
    "    UserId STRING,\n",
    "    ItemId STRING,\n",
    "    ItemName STRING,\n",
    "    ItemCategory STRING,\n",
    "    Price STRING,\n",
    "    CreatedAt STRING,\n",
    "    PaidAt STRING\n",
    "                 );\n",
    "                 \n",
    "CREATE TABLE item_raw(                \n",
    "    adjective STRING,\n",
    "    category STRING,\n",
    "    created_at STRING,\n",
    "    id STRING,\n",
    "    modifier STRING,\n",
    "    name STRING,\n",
    "    price STRING  \n",
    "                 );            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd4aee",
   "metadata": {},
   "source": [
    "### LAYER 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f9a9c",
   "metadata": {},
   "source": [
    "***▪ Contains all datasets from the first layer \\\n",
    "▪ All attributes have common naming convention \\\n",
    "▪ All attributes have proper datatypes based on the attribute name and\n",
    "common logic\\\n",
    "▪ All struct collection attributes are flattened and transformed to proper data\n",
    "types\\\n",
    "▪ Fact tables are properly partitioned based on meaningful attributes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to Check\n",
    "# expload at event.payload\n",
    "# {\"event_name\":\"view_item\",\"platform\":\"android\",\"parameter_name\":\"item_id\",\"parameter_value\":\"3526\"}\n",
    "# Fact table is needed to create  \n",
    "    \n",
    "\n",
    "CREATE TABLE event_raw_input AS(\n",
    "    \n",
    "     SELECT \n",
    "             event_id STRING,\n",
    "             CAST(event_time AS TIMESTAMP(‘MM/DD/YYYY HH:MI:ss) ) event_time,\n",
    "             CAST(user_id AS UNSIGNED ) user_id, \n",
    "             CAST expload(event.payload AS STRING) event,\n",
    "             CAST(event.event_name.value AS STRING) event_name,\n",
    "             CAST(event.platform.value AS STRING) platform,\n",
    "             CAST(event.parameter_name.value AS STRING) parameter_name,\n",
    "            \n",
    "             \n",
    "    FROM event_raw\n",
    "                              );\n",
    "                 \n",
    "CREATE TABLE user_raw_input AS(\n",
    "    \n",
    "    SELECT\n",
    "        CAST(created_at AS  TIMESTAMP(‘MM/DD/YYYY HH:MI:ss)) created_at ,\n",
    "        CAST(deleted_at AS  TIMESTAMP(‘MM/DD/YYYY HH:MI:ss) ) deleted_at,\n",
    "        email_address STRING,\n",
    "        first_name STRING,\n",
    "        CAST(id AS UNSIGNED) user_id, \n",
    "        last_name STRING,\n",
    "        CAST(merged_at AS  TIMESTAMP(‘MM/DD/YYYY HH:MI:ss)) merged_at,\n",
    "        CAST(parent_user_id AS UNSIGNED) parent_user_id\n",
    "    FROM user_raw \n",
    "                     );\n",
    "\n",
    "\n",
    "CREATE TABLE order_raw_input( \n",
    "    \n",
    "    SELECT\n",
    "            CAST(InvoiceId AS UNSIGNED) invoice_id,\n",
    "            CAST(LineItemId AS UNSIGNED) line_item_id,\n",
    "            CAST(UserId AS UNSIGNED) user_id\n",
    "            CAST(ItemId AS UNSIGNED) item_id,\n",
    "            CAST(ItemName AS STRING) item_name,\n",
    "            CAST(ItemCategory AS STRING) item_category,\n",
    "            CAST(Price AS FLOAT) price,\n",
    "            CAST(CreatedAt AS TIMESTAMP(‘MM/DD/YYYY HH:MI:ss)) created_at ,\n",
    "            CAST(PaidAt AS TIMESTAMP(‘MM/DD/YYYY HH:MI:ss)) paid_at ,\n",
    "    FROM order_raw\n",
    "                            );\n",
    "                 \n",
    "CREATE TABLE item_raw_input(  \n",
    "    \n",
    "    SELECT\n",
    "            adjective STRING,\n",
    "            CAST(category AS STRING) item_category,\n",
    "            CAST(created_at AS TIMESTAMP(‘MM/DD/YYYY HH:MI:ss)) created_at ,\n",
    "            CAST(id AS UNSIGNED) item_id,\n",
    "            modifier STRING,\n",
    "            CAST(name AS STRING) item_name,\n",
    "           CAST(price AS FLOAT) price \n",
    "    FROM item_raw\n",
    "                            );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de69705",
   "metadata": {},
   "source": [
    "### LAYER 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec590f3d",
   "metadata": {},
   "source": [
    "***▪ Contains following data marts:***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a3014",
   "metadata": {},
   "source": [
    "***• “top_items” data mart with all sold items with additional attributes:***\\\n",
    "o For every year (based on the created_at attribute):\\\n",
    "     ▪ Total number of an items sold in a particular year\\\n",
    "     ▪ Rank of an item based on the total number of items sold in a particular year\\\n",
    "     ▪ Total sales from an item in a particular year\\\n",
    "     ▪ Rank of an item based on the total sales in a particular year\\\n",
    "o Total number of items sold in all years\\\n",
    "o Rank of an item based on the total number of solds\\\n",
    "o Total sales of an item in all years\\\n",
    "o Rank of an item based on the total sales\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c7eb3",
   "metadata": {},
   "source": [
    "### For every year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09aa650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df.select('item_id','item_name','line_item_id','created_at') where year == 2021\n",
    "\n",
    "# # ▪ Total number of an items sold in a particular year\n",
    "# # ▪ Rank of an item based on the total number of items sold in a particular year\n",
    "\n",
    "# total_sold=spark_df.groupBy([\"item_name\"]).agg(\n",
    "#     count(\"item_name\").alias(\"total_items_sold\"))\n",
    "# rank_of_item = total_sold.sort(desc('rank_of_item')) \n",
    "\n",
    "# # ▪ Total sales from an item in a particular year\n",
    "# # ▪ Rank of an item based on the total sales in a particular year\n",
    "\n",
    "# total_sales=spark_df.groupBy([\"item_name\"]).agg(\n",
    "#    sum(\"price\").alias(\"total_items_sales\"))\n",
    "# rank_of_item = total_sold.sort(desc('rank_of_item')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02b226",
   "metadata": {},
   "source": [
    "### For all year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_df.select('item_id','item_name','line_item_id','created_at') where year == 2021\n",
    "\n",
    "# # o Total number of items sold in all years\n",
    "# # o Rank of an item based on the total number of solds\n",
    "\n",
    "\n",
    "# total_sold=spark_df.groupBy([\"item_name\"]).agg(\n",
    "#     count(\"item_name\").alias(\"total_items_sold\"))\n",
    "# rank_of_item = total_sold.sort(desc('rank_of_item')) \n",
    "\n",
    "# # o Total sales of an item in all years\n",
    "# # o Rank of an item based on the total sales\n",
    "\n",
    "# total_sales=spark_df.groupBy([\"item_name\"]).agg(\n",
    "#    sum(\"price\").alias(\"total_items_sales\"))\n",
    "# rank_of_item = total_sold.sort(desc('rank_of_item')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d7ff8",
   "metadata": {},
   "source": [
    "***• “top_buyers” data mart with top 20 customers who contributed on the total sales the most with additional attributes:***\\\n",
    "o Total sales contributed\\\n",
    "o Rank based on the total sales\\\n",
    "o Last order creation date\\\n",
    "o The overall most viewed item of a customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88566fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users=spark_df.groupBy([\"user_id\"]).agg(count(\"item_id\"),sum('price').alias(\"items_order_by_user\"))\n",
    "# rank_of_item = total_sold.sort(desc('items_order_by_user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707bd83",
   "metadata": {},
   "source": [
    "### Read data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd71f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(PATH+'user'+API_SUFFIX, inferSchema = True, header = True)\n",
    "df.show(6,False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48449144",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW=\"_raw\"\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sqlCtx = HiveContext(sc)\n",
    "for name in NAME_ARRAY:\n",
    "    spark_df= sqlCtx.read.format('csv').options(header='true', inferSchema='true').load(S3_PATH+name+API_SUFFIX)\n",
    "    spark_df.registerTempTable(name)\n",
    "    sqlCtx.sql(f\"CREATE TABLE {name+RAW} AS SELECT * FROM {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a3088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
